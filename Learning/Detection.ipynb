{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-akm/Loan_Class_App/blob/main/Learning/Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "2R23DIptdEfw",
        "outputId": "a679e73e-e585-4939-a192-4d2242e6e777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a76b8b46-1887-43d5-ac49-9edf267ae4a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a76b8b46-1887-43d5-ac49-9edf267ae4a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"danielaungkhantmoe\",\"key\":\"80682f47678f298fdd26a784977ccbc1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "stQ7WJaExAbm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "\n",
        "# Move the file to the correct directory\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "swjWXsiIeULf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IrBcLe9Su5s",
        "outputId": "003fd302-496c-4440-8a9a-81f1d008ff13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                              title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  -------------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "birdy654/deep-voice-deepfake-voice-recognition                   DEEP-VOICE: DeepFake Voice Recognition              3956840339  2023-08-24 13:12:23.837000          11447        103  1.0              \n",
            "manjilkarki/deepfake-and-real-images                             deepfake and real images                            1808722718  2022-02-03 15:33:45.663000          30038        121  0.625            \n",
            "abdallamohamed312/in-the-wild-audio-deepfake                     In The Wild (audio Deepfake)                                 0  2024-04-20 04:28:55.640000           1139         48  1.0              \n",
            "dagnelies/deepfake-faces                                         deepfake_faces                                       454127868  2020-02-02 19:44:24.663000           7607         85  0.5882353        \n",
            "khoongweihao/deepfake-xception-trained-model                     Deepfake Xception Trained Model                      489778189  2020-02-25 01:12:29.703000           1311         20  0.8125           \n",
            "saurabhbagchi/deepfake-image-detection                           Deepfake image detection                             499633871  2025-01-15 05:49:42.057000           2668         13  0.75             \n",
            "unkownhihi/deepfake                                              DeepFake(150*150)                                    800415004  2020-01-15 22:28:29.863000           1020         35  0.375            \n",
            "ymirsky/medical-deepfakes-lung-cancer                            Medical Deepfakes: Lung Cancer                      6432956210  2020-04-23 17:50:12.857000           3710         55  0.7941176        \n",
            "khoongweihao/deepfake-kernel-data                                Deepfake Kernel Data                                  81716326  2020-02-28 13:47:55.250000            204          8  0.8125           \n",
            "robikscube/deepfakemodelspackages                                deepfake-models-packages                             587482860  2019-12-16 16:32:12.653000            646         30  0.375            \n",
            "vlbthambawita/deepfake-ecg                                       DeepFake ECG                                       16548382130  2021-05-27 20:47:05.553000            363         20  0.8235294        \n",
            "mohammedabdeldayem/the-fake-or-real-dataset                      The Fake-or-Real (FoR) Dataset (deepfake audio)    17211773247  2024-04-16 01:54:50.233000           6769         55  0.75             \n",
            "phunghieu/deepfake-detection-faces-part-0-0                      Deepfake Detection - Faces - Part 0_0               4751090848  2020-02-04 03:08:49.373000           1429         39  0.8235294        \n",
            "humananalog/deepfakes-inference-demo                             Deepfakes Inference Demo                              84874507  2020-01-21 12:51:52.470000           2387         40  0.5              \n",
            "phunghieu/deepfake-detection-logistic-regression                 Deepfake Detection - Logistic Regression                   567  2020-03-05 00:26:34.620000           1276         18  1.0              \n",
            "sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset  Deep Fake Detection (DFD) Entire Original Dataset  24156501909  2024-08-10 11:48:58.853000           9886         70  0.8125           \n",
            "peilwang/deepfake                                                deepfake                                           24678032097  2024-07-03 08:31:00.880000            572          7  0.1875           \n",
            "greatgamedota/faceforensics                                      FaceForensics                                        724841395  2020-02-19 17:42:41.720000           3028         32  0.75             \n",
            "moutasmtamimi/fake-news-dataset                                  Fake News Dataset                                       922199  2025-05-28 17:30:03.800000            770         29  0.7058824        \n",
            "mohammedabdeldayem/avsspoof-2021                                 ASVspoof 2021 Dataset                              62250567017  2024-05-18 17:40:31.800000           1066         27  0.875            \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list -s deepfake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRHxABKvSyMC",
        "outputId": "ad09021b-1faa-4f2c-f002-f261e874518a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n",
            "License(s): unknown\n",
            "deepfake-and-real-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d manjilkarki/deepfake-and-real-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdP-rqRHS1l_",
        "outputId": "9eeb37e6-e34e-4e1f-9924-d3906fdb8b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  deepfake-and-real-images.zip\n",
            "replace deepfake-and-real-images/Dataset/Test/Fake/fake_0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: deepfake-and-real-images/Dataset/Test/Fake/fake_0.jpg  \n",
            "replace deepfake-and-real-images/Dataset/Test/Fake/fake_1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip deepfake-and-real-images.zip -d deepfake-and-real-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yGuawVWTJZA"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Remove existing folder if it exists\n",
        "shutil.rmtree(\"deepfake-and-real-images\", ignore_errors=True)\n",
        "\n",
        "# Extract the ZIP file again\n",
        "with zipfile.ZipFile(\"deepfake-and-real-images.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"deepfake-and-real-images\")\n",
        "\n",
        "# Confirm extraction\n",
        "import os\n",
        "print(os.listdir(\"deepfake-and-real-images\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pSK1dngTMVk"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbP23IpRTPQN"
      },
      "outputs": [],
      "source": [
        "data_path = Path('deepfake-and-real-images')\n",
        "image_path = data_path / \"Dataset\"\n",
        "train_path = image_path / \"Train\"\n",
        "test_path = image_path / \"Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv3TpjZHTaSO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "#set seed\n",
        "#random.seed(42)\n",
        "\n",
        "#Get all image paths\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "#pick a random image path\n",
        "random_image_path = random.choice(image_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WubmqFZ6TbEP"
      },
      "outputs": [],
      "source": [
        "image_class = random_image_path.parent.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hXcOfH7TgdD"
      },
      "outputs": [],
      "source": [
        "img = Image.open(random_image_path)\n",
        "#print metadata\n",
        "print(f'Random Image Path: {random_image_path}')\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f'Image Heigh: {img.height}')\n",
        "print(f'Image Width: {img.width}')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVhB9MFATi_G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irbmbRpmTmTM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "img_as_array = np.asarray(img)\n",
        "plt.imshow(img)\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQfHarNkTpD9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(size=(256,256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomCrop(size=(224,224)),\n",
        "    transforms.ColorJitter(\n",
        "        brightness = 0.2,\n",
        "        contrast = 0.2,\n",
        "        saturation = 0.2,\n",
        "        hue = 0.2\n",
        "    ),\n",
        "    transforms.RandomPerspective(p=0.5),\n",
        "    transforms.RandomAffine(degrees=30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def plot_transformed_images(image_paths, transforms, n=3, seed = 42):\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "  random_image_paths = random.sample(image_paths, k=n)\n",
        "  for image_path in random_image_paths:\n",
        "    with Image.open(image_path) as f:\n",
        "      fig, ax = plt.subplots(nrows = 1, ncols = 2)\n",
        "      ax[0].imshow(f)\n",
        "      ax[0].set_title(f'Original SIze: {f.size}')\n",
        "      ax[0].axis(False)\n",
        "\n",
        "      transformed_image = transforms(f).permute(1,2,0)\n",
        "      ax[1].imshow(transformed_image)\n",
        "      ax[1].set_title(f'Transformed Shape: {transformed_image.shape}')\n",
        "      ax[1].axis(False)\n",
        "      fig.suptitle(f'Class :{image_path.parent.stem}', fontsize = 16)\n",
        "plot_transformed_images(image_path_list, data_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLjVFMBUTyER"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.ImageFolder(root = train_path,\n",
        "                                  transform= data_transforms,\n",
        "                                  target_transform=None)\n",
        "test_data = datasets.ImageFolder(root = test_path,\n",
        "                                 transform = data_transforms)\n",
        "classes = train_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwpjCLxT3VS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset=train_data, batch_size = 8, num_workers=1, shuffle = True) # Reduced batch size to 16\n",
        "test_dataloader = DataLoader(dataset = test_data, batch_size = 8, num_workers=1, shuffle = False) # Reduced batch size to 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFbIR2dKT9hc"
      },
      "outputs": [],
      "source": [
        "class DeepfakeDetectionModel(nn.Module):\n",
        "  def __init__(self, in_features, out_features, hidden_units):\n",
        "    super().__init__()\n",
        "    #We need multiple blocks for out data to pass through\n",
        "    self.block1 = nn.Sequential(\n",
        "        #First the convuloutional layer\n",
        "        nn.Conv2d(in_channels=in_features,\n",
        "                  out_channels = hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride = 1,\n",
        "                  padding=0),\n",
        "        nn.BatchNorm2d(hidden_units),  # Add Batch Normalization\n",
        "        ##ReLU layer\n",
        "        nn.LeakyReLU(),\n",
        "        #Maxpool Layer\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride = 2))\n",
        "    #Same block\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride = 1,\n",
        "                  padding = 0),\n",
        "        nn.BatchNorm2d(hidden_units),  # Add Batch Normalization\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels = hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride = 1,\n",
        "                  padding = 0),\n",
        "        nn.BatchNorm2d(hidden_units),  # Add Batch Normalization\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride = 2))\n",
        "\n",
        "    self.dropout = nn.Dropout(0.4)  # Add dropout to prevent overfitting\n",
        "\n",
        "    # Dummy forward pass to calculate the flattened size\n",
        "    dummy_input = torch.randn(1, in_features, 224, 224) # Assuming input image size is 224x224\n",
        "    dummy_output = self.block1(dummy_input)\n",
        "    dummy_output = self.block2(dummy_output)\n",
        "    flattened_size = dummy_output.view(dummy_output.size(0), -1).shape[1]\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        # Use the calculated flattened_size for the first linear layer\n",
        "        nn.Linear(in_features = flattened_size, out_features=out_features),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(0.5),  # Add another dropout\n",
        "        nn.Linear(out_features, out_features)\n",
        "    )\n",
        "  #Forward pass the data through the different layers, and return the final output of x\n",
        "  def forward(self,x):\n",
        "      x = self.block1(x)\n",
        "      x = self.block2(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.classifier(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUStW3WbeLIs"
      },
      "outputs": [],
      "source": [
        "len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4FfPDgscjVa"
      },
      "outputs": [],
      "source": [
        "model = DeepfakeDetectionModel(in_features=3, out_features=1, hidden_units=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-EYEbbYeupG"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_dataloader)) #Create dummy data do find out what the value needs to be for the input shape of our linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18dqv89vfC1D"
      },
      "outputs": [],
      "source": [
        "image_batch.shape, label_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_fbpVkefSDw"
      },
      "outputs": [],
      "source": [
        "model(image_batch.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILtnLNLCsdjR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "WEIGHT_DECAY = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcqVcWUTsdjS"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
        "# TODO  - change the scheduler accordingly\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nllwfsLvsdjS"
      },
      "outputs": [],
      "source": [
        "def train_one_step(model, dataloader, optimizer, scheduler, loss, device):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for image_batch, label_batch in dataloader:\n",
        "    image_batch = image_batch.to(device)\n",
        "    label_batch = label_batch.to(device)\n",
        "    pred = model(image_batch)\n",
        "    loss = loss_fn(pred, label_batch.unsqueeze(1).float())\n",
        "    total_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  return total_loss/len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculcate_accuracy(pred, true):\n",
        "  preds = torch.round(torch.sigmoid(pred)) # Apply sigmoid to get probabilities and round to 0 and 1\n",
        "  accuracy = (preds == true).float().mean() # Compare the results and find the average\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "A_12qoNCVIk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqJ_1hc0sdjS"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, loss_fn, device):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  #Context manager to disable gradient calculation during testing\n",
        "  with torch.no_grad(): #This will stop pytorch from storing gradients during evaluation\n",
        "    for image_batch, label_batch in dataloader:\n",
        "      image_batch = image_batch.to(device)\n",
        "      label_batch = label_batch.to(device)\n",
        "      pred = model(image_batch)\n",
        "      loss += loss_fn(pred, label_batch.unsqueeze(1).float()).item() #Call .item() to move the loss to the CPU and save memory on the GPU\n",
        "      accuracy += calculcate_accuracy(pred, label_batch) # Calculate the accuracy\n",
        "  return loss/len(dataloader), accuracy/len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkbgDlns25uy"
      },
      "outputs": [],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPwu9zalsdjS"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def train(model, train_dataloader, test_dataloader, optimizer, scheduler, loss_fn, epochs, device):\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    loss = train_one_step(model, train_dataloader, optimizer, scheduler, loss_fn, device)\n",
        "    test_loss, accuracy = test(model, test_dataloader, loss_fn, device)\n",
        "    print(f\"Epoch: [{epoch+1}/{epochs}] | Loss: {loss} | Test Loss: {test_loss} | Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21xrSmBWsdjS"
      },
      "outputs": [],
      "source": [
        "# Accuracy added\n",
        "train(model, train_dataloader, test_dataloader, optimizer, scheduler, loss_fn, EPOCHS, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}